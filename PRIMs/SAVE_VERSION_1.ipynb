{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What works\n",
    "\n",
    "- Perform experiment with all conditions\n",
    "- almost everything correct after learning\n",
    "- same when loading weights again\n",
    "\n",
    "\n",
    "Learning parameters (rest has not been changed for testing weight loading):\n",
    "\n",
    "n_blocks_per_operation = 5 # default: 5\n",
    "\n",
    "n_digits_per_block = 4+1 # default: 4\n",
    "\n",
    "n_different_digits = 4 # default: 4\n",
    "\n",
    "n_different_operations = 3 # default: 3\n",
    "\n",
    "\n",
    "\n",
    "number_of_total_trials = n_blocks_per_operation * n_digits_per_block * n_different_operations * n_different_digits\n",
    "\n",
    "number_of_non_learning_trials = 40 #number_of_total_trials\n",
    "\n",
    "number_of_learning_trials = max(0,number_of_total_trials - number_of_non_learning_trials)\n",
    "\n",
    "print(\"number_of_learning_trials\",number_of_learning_trials) \n",
    "\n",
    "print(\"number_of_non_learning_trials\",number_of_non_learning_trials) \n",
    "\n",
    "print(\"number_of_total_trials\",number_of_total_trials)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "number_of_learning_trials 0\n",
    "\n",
    "number_of_non_learning_trials 40\n",
    "\n",
    "number_of_total_trials 12\n",
    "\n",
    "T 24.34799\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nengo\n",
    "import nengo_spa as spa\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import random\n",
    "from weight_save import WeightSaver\n",
    "\n",
    "\n",
    "use_ocl = True\n",
    "if use_ocl:\n",
    "    import nengo_ocl\n",
    "    simulator = nengo_ocl.Simulator\n",
    "else:\n",
    "    simulator = nengo.Simulator\n",
    "    \n",
    "import sys, os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trial():\n",
    "    def __init__(self, operation, stimulus):\n",
    "        self.operation = operation\n",
    "        self.stimulus = stimulus\n",
    "\n",
    "class Experiment():\n",
    "    def __init__(self, trial_length, number_of_learning_trials, trials):\n",
    "        self.trial_length = trial_length\n",
    "        self.number_of_learning_trials = number_of_learning_trials\n",
    "        self.trials = trials\n",
    "\n",
    "    def __call__(self, t):\n",
    "        t = round(t,4) - .001 # Avoid float problems\n",
    "        trial_number = math.floor(t / self.trial_length)\n",
    "        t_in_trial = t - trial_number * self.trial_length\n",
    "        trial = self.trials[trial_number]\n",
    "        return trial, t_in_trial\n",
    "\n",
    "    \n",
    "    def RETINA_input(self, t):\n",
    "        trial, t_in_trial = self(t)\n",
    "        \n",
    "        if 1 < t_in_trial:# < 1.029:\n",
    "            return trial.stimulus\n",
    "        else:\n",
    "            return \"0\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def RETINA_input(self, t):\n",
    "        trial, t_in_trial = self(t)\n",
    "        SOA = .029\n",
    "        if 1 < t_in_trial < .016:\n",
    "            return trial.stimulus\n",
    "        elif 1+SOA < t_in_trial < 1+SOA+.150: # Masks during 150 ms after SOA (=stim + fixation)\n",
    "            return \"0\"\n",
    "            return \"X\"\n",
    "        else:\n",
    "            return \"0\"\n",
    "    \"\"\"\n",
    "\n",
    "    def G_input(self, t):\n",
    "        trial = self(t)[0]\n",
    "        return trial.operation\n",
    "\n",
    "    def CORRECT_PRIM_input(self, t):\n",
    "        trial, t_in_trial = self(t)\n",
    "\n",
    "        # before stimulus appears\n",
    "        if t_in_trial < 1:\n",
    "            return 'FOCUS'\n",
    "\n",
    "        else: # ['SIMPLE', 'CHAINED_ADD', 'CHAINED_SUB']\n",
    "            if trial.operation == 'SIMPLE':\n",
    "                if   1 < t_in_trial < 1.33:\n",
    "                    return 'V_COM'\n",
    "                elif 1.33 < t_in_trial < 1.66:\n",
    "                    return 'COM_PM'\n",
    "                else:\n",
    "                    return '0'\n",
    "\n",
    "            elif trial.operation == 'CHAINED_ADD':\n",
    "                if   1 < t_in_trial < 1.33:\n",
    "                    return 'V_ADD'\n",
    "                elif 1.33 < t_in_trial < 1.66:\n",
    "                    return 'ADD_COM'\n",
    "                elif 1.66 < t_in_trial < 1.99:\n",
    "                    return 'COM_PM'\n",
    "                else:\n",
    "                    return '0'\n",
    "\n",
    "            elif trial.operation == 'CHAINED_SUB':\n",
    "                if   1 < t_in_trial < 1.33:\n",
    "                    return 'V_SUB'\n",
    "                elif 1.33 < t_in_trial < 1.66:\n",
    "                    return 'SUB_COM'\n",
    "                elif 1.66 < t_in_trial < 1.99:\n",
    "                    return 'COM_PM'\n",
    "                return '0'\n",
    "\n",
    "            else:\n",
    "                print(\"unknown operation\")\n",
    "\n",
    "    def learning_inhibit_input(self, t):\n",
    "        trial_number = math.floor(t / self.trial_length)\n",
    "        if trial_number < self.number_of_learning_trials:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "class RandomExperiment(Experiment):\n",
    "    def __init__(self, trial_length, n_blocks_per_operation, n_digits_per_block):\n",
    "        trials = []\n",
    "        for operation in ['SIMPLE', 'CHAINED_ADD', 'CHAINED_SUB']:\n",
    "            for i in range(n_blocks_per_operation*n_digits_per_block):\n",
    "                for stimulus in ['TWO', 'FOUR', 'SIX', 'EIGHT']:\n",
    "                    trials.append(Trial(operation, stimulus))\n",
    "        shuffle(trials)\n",
    "        \n",
    "        super().__init__(trial_length, number_of_learning_trials, trials)\n",
    "        \n",
    "class SimpleRandomExperiment(Experiment):\n",
    "    def __init__(self, trial_length, n_blocks_per_operation, n_digits_per_block, n_different_digits=4):\n",
    "        trials = []\n",
    "        for operation in ['SIMPLE']:\n",
    "            for i in range(n_blocks_per_operation*n_digits_per_block):\n",
    "                for stimulus in ['TWO', 'FOUR', 'SIX', 'EIGHT'][:n_different_digits]:\n",
    "                    trials.append(Trial(operation, stimulus))\n",
    "        shuffle(trials)\n",
    "        \n",
    "        super().__init__(trial_length, number_of_learning_trials, trials)\n",
    "\n",
    "class Button():\n",
    "    def __init__(self, SP_vectors, trial_length, dt=None, thr=.5, focus_length=1):\n",
    "        self.t_last_evt = -100\n",
    "        self.SP_vectors = SP_vectors\n",
    "        self.t_last_step = 0\n",
    "        self.dt = dt\n",
    "        self.thr = thr\n",
    "        self.trial_length = trial_length\n",
    "        self.focus_length = focus_length\n",
    "    \n",
    "    def __call__(self,t,x):\n",
    "        if not self.dt or t-self.dt > self.t_last_step:\n",
    "            self.t_last_step = t\n",
    "            if t//self.trial_length > self.t_last_evt//self.trial_length and t > (t//self.trial_length)*self.trial_length + self.focus_length:\n",
    "                for i in range(len(self.SP_vectors)):\n",
    "                    similarities = np.dot(self.SP_vectors,x)\n",
    "                    if np.dot(x,self.SP_vectors[i]) > self.thr:\n",
    "                        self.t_last_evt = t\n",
    "                        return i+1\n",
    "                        \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_learning_trials 0\n",
      "number_of_non_learning_trials 40\n",
      "number_of_total_trials 12\n",
      "T 24.34799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=12, similarity=0.12)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=13, similarity=0.14)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=14, similarity=0.13)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=15, similarity=0.15)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=16, similarity=0.14)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=17, similarity=0.12)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=18, similarity=0.17)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=19, similarity=0.19)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=20, similarity=0.16)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=21, similarity=0.18)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=22, similarity=0.20)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=23, similarity=0.20)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=24, similarity=0.18)\n",
      "  len(self._key2idx), best_sim))\n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "s = spa.sym\n",
    "D = 32  # the dimensionality of the vectors\n",
    "AM_THR = .15\n",
    "ROUTING_THR = .15\n",
    "GW_threshold = 0\n",
    "\n",
    "# Number of neurons (per dimension or ensemble)\n",
    "scale_npds = 1\n",
    "npd_AM = int(50*scale_npds) # Default: 50\n",
    "npd_state = int(50*scale_npds) # Default: 50\n",
    "npd_BG = int(100*scale_npds) # Default: 100\n",
    "npd_thal1 = int(50*scale_npds) # Default: 50\n",
    "npd_thal2 = int(40*scale_npds) # Default: 40\n",
    "n_scalar = int(50*scale_npds) # Default: 50\n",
    "\n",
    "save_weights = False\n",
    "load_weights = True\n",
    "\n",
    "n_blocks_per_operation = 1#5 # default: 5\n",
    "n_digits_per_block = 1#4+1 # default: 4\n",
    "n_different_digits = 4 # default: 4\n",
    "n_different_operations = 3 # default: 3\n",
    "\n",
    "number_of_total_trials = n_blocks_per_operation * n_digits_per_block * n_different_operations * n_different_digits\n",
    "number_of_non_learning_trials = 40 #number_of_total_trials\n",
    "number_of_learning_trials = max(0,number_of_total_trials - number_of_non_learning_trials)\n",
    "print(\"number_of_learning_trials\",number_of_learning_trials) \n",
    "print(\"number_of_non_learning_trials\",number_of_non_learning_trials) \n",
    "print(\"number_of_total_trials\",number_of_total_trials)\n",
    "\n",
    "\n",
    "trial_length = 2.029\n",
    "\n",
    "T = number_of_total_trials * trial_length - .00001# simulations run a bit too long\n",
    "print('T',T)\n",
    "\n",
    "symbol_keys = ['TWO','FOUR','SIX','EIGHT','X', \\\n",
    "               'MORE','LESS', \\\n",
    "    'G', 'V', 'COM', 'ADD', 'SUB', 'PREV', 'PM', \\\n",
    "    'SIMPLE', 'CHAINED_ADD', 'CHAINED_SUB'\n",
    "    ]\n",
    "prim_keys = ['V_COM', 'COM_PM', 'V_ADD', 'V_SUB', 'ADD_COM', 'SUB_COM', 'V_PM', 'FOCUS']\n",
    "all_keys = symbol_keys + prim_keys\n",
    "vocab_memory = spa.Vocabulary(dimensions=D, name='all', pointer_gen=np.random.RandomState(seed))\n",
    "vocab_memory.populate(\";\".join(all_keys))\n",
    "prim_vocab = vocab_memory.create_subset(prim_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from file\n",
      "Loading weights from file\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "model = spa.Network(seed=seed)\n",
    "with model:\n",
    "    \n",
    "    model.config[spa.State].neurons_per_dimension = npd_state\n",
    "    #model.config[spa.WTAAssocMem].n_neurons = npd_AM # Doesn't work -> set for individual AM\n",
    "    model.config[spa.Scalar].n_neurons = n_scalar\n",
    "    model.config[spa.BasalGanglia].n_neurons_per_ensemble = npd_BG\n",
    "    model.config[spa.Thalamus].neurons_action = npd_thal1\n",
    "    model.config[spa.Thalamus].neurons_channel_dim = npd_thal1\n",
    "    model.config[spa.Thalamus].neurons_gate = npd_thal2\n",
    "\n",
    "    # We start defining the buffer slots in which information can\n",
    "    # be placed:\n",
    "    \n",
    "    # A slot for the goal/task\n",
    "    G = spa.State(vocab_memory, label='G')\n",
    "    \n",
    "    # A slot for the visual input (the digit N). Feedback is used for iconic memory (100-300ms)\n",
    "    RETINA = spa.WTAAssocMem(\n",
    "        0.1,\n",
    "        vocab_memory,\n",
    "        mapping=['TWO','FOUR','SIX','EIGHT','X'],\n",
    "        function=lambda x: x>0,\n",
    "        n_neurons = npd_AM\n",
    "    )\n",
    "    nengo.Connection(RETINA.input, RETINA.input, transform=.85, synapse=.005)\n",
    "    V = spa.State(vocab_memory, label='V')\n",
    "    nengo.Connection(RETINA.output, V.input, synapse=.055)\n",
    "    \n",
    "    # The previously executed PRIM\n",
    "    PREV = spa.State(vocab_memory, feedback=.95, feedback_synapse=.05, label='PREV')\n",
    "    \n",
    "    # A slot for the action (MORE or LESS)\n",
    "    PM = spa.State(vocab_memory, feedback=.8, feedback_synapse=.05, label='PM')\n",
    "    with nengo.Network() as ACT_net:\n",
    "        ACT_net.config[nengo.Ensemble].neuron_type = nengo.Direct()\n",
    "        ACT = spa.State(vocab_memory, label='ACT direct')\n",
    "\n",
    "    # An associative memory for the + operation\n",
    "    ADD_input = spa.State(vocab_memory, feedback=.8, feedback_synapse=.05, label='ADD_input')\n",
    "    ADD = spa.WTAAssocMem(threshold=AM_THR, \n",
    "        input_vocab=vocab_memory, mapping=\n",
    "        {\n",
    "            'TWO':'FOUR',\n",
    "            'FOUR':'SIX',\n",
    "            'SIX':'EIGHT',\n",
    "            'EIGHT':'TWO',\n",
    "        },\n",
    "        function=lambda x: x>0,\n",
    "        label='ADD',\n",
    "        n_neurons = npd_AM\n",
    "    )\n",
    "    ADD_input >> ADD.input\n",
    "    \n",
    "    # An associative memory for the - operation\n",
    "    SUB_input = spa.State(vocab_memory, feedback=.8, feedback_synapse=.05, label='SUB_input')\n",
    "    SUB = spa.WTAAssocMem(threshold=AM_THR, \n",
    "        input_vocab=vocab_memory, mapping=\n",
    "        {\n",
    "            'TWO':'EIGHT',\n",
    "            'FOUR':'TWO',\n",
    "            'SIX':'FOUR',\n",
    "            'EIGHT':'SIX',\n",
    "        },\n",
    "        function=lambda x: x>0,\n",
    "        label='SUB',\n",
    "        n_neurons = npd_AM\n",
    "    )\n",
    "    SUB_input >> SUB.input\n",
    "    \n",
    "    # An associative memory for the \"compare to 5\" operation\n",
    "    COM_input = spa.State(vocab_memory, feedback=.8, feedback_synapse=.05, label='COM_input')\n",
    "    COM = spa.WTAAssocMem(threshold=AM_THR, \n",
    "        input_vocab=vocab_memory, mapping=\n",
    "        {\n",
    "            'TWO':'LESS',\n",
    "            'FOUR':'LESS',\n",
    "            'SIX':'MORE',\n",
    "            'EIGHT':'MORE',\n",
    "        },\n",
    "        function=lambda x: x>0,\n",
    "        label='COM',\n",
    "        n_neurons = npd_AM\n",
    "    )\n",
    "    COM_input >> COM.input\n",
    "\n",
    "    # A slot that combines selected information from the processors\n",
    "    \"\"\"GW = spa.State(vocab_memory, neurons_per_dimension = 150, label='GW')\"\"\"\n",
    "    GW = spa.State(vocab_memory, neurons_per_dimension = 150, label='GW', represent_cc_identity=False)\n",
    "    processors = [G, V, PREV, PM, ADD, SUB, COM]\n",
    "    competition_keys = {\n",
    "        G: ['SIMPLE', 'CHAINED_ADD', 'CHAINED_SUB'],\n",
    "        V: ['TWO','FOUR','SIX','EIGHT','X'],\n",
    "        PREV: ['V_COM', 'COM_PM', 'V_ADD', 'V_SUB', 'ADD_COM', 'SUB_COM', 'V_PM', 'FOCUS'],\n",
    "        PM: ['MORE','LESS'],\n",
    "        ADD: ['TWO','FOUR','SIX','EIGHT'],\n",
    "        SUB: ['TWO','FOUR','SIX','EIGHT'],\n",
    "        COM: ['MORE','LESS'],\n",
    "    }\n",
    "    for processor in processors:\n",
    "        source = processor.output\n",
    "        if GW_threshold:\n",
    "            proc_threshold = spa.modules.WTAAssocMem(\n",
    "                GW_threshold,\n",
    "                vocab_memory,\n",
    "                mapping=competition_keys[processor],\n",
    "                function=lambda x: x>0,\n",
    "                n_neurons = npd_AM\n",
    "            )\n",
    "            processor >> proc_threshold.input\n",
    "            source = proc_threshold.output\n",
    "            \n",
    "        nengo.Connection(source, GW.input, \n",
    "            transform=vocab_memory.parse(processor.label).get_binding_matrix())\n",
    "\n",
    "    # The PRIM state will receive the PRIM to be executed based on GW\n",
    "    PRIM = spa.State(prim_vocab, label='PRIM')\n",
    "    \n",
    "    # We will do supervised learned, so this state will hold the correct PRIM to execute\n",
    "    CORRECT_PRIM = spa.State(prim_vocab, label='correct action')\n",
    "    \n",
    "    # This state will be used to calculate the error\n",
    "    error = spa.State(prim_vocab, label='error')\n",
    "    PRIM - CORRECT_PRIM >> error\n",
    "    \n",
    "    \n",
    "    # Create learning connection between GW and PRIM\n",
    "    weightSavers = []\n",
    "    folder_name = \"weights\"\n",
    "    for i,ens in enumerate(GW.all_ensembles):\n",
    "        \n",
    "        con = nengo.Connection(ens, PRIM.input, function=lambda t: [0]*D,\n",
    "                               learning_rule_type=nengo.PES(learning_rate=1e-4)) # was 1e-4 and changed to 1e-5\n",
    "        nengo.Connection(error.output, con.learning_rule)\n",
    "        ws = WeightSaver(con, folder_name+'/w'+str(i), load=load_weights, sample_every=T)\n",
    "        weightSavers.append(ws)\n",
    "\n",
    "    \n",
    "    if n_different_operations==3:\n",
    "        xp = RandomExperiment(trial_length, n_blocks_per_operation, n_digits_per_block)\n",
    "    elif n_different_operations==1:\n",
    "        xp = SimpleRandomExperiment(trial_length, n_blocks_per_operation, n_digits_per_block, n_different_digits)\n",
    "    else:\n",
    "        print('Unknown experiment')    \n",
    "    \n",
    "    \"\"\" \n",
    "    xp = RandomExperiment(trial_length, n_blocks_per_operation, n_digits_per_block)\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Create the inputs\n",
    "    with spa.Network(label='inputs'):\n",
    "        RETINA_input = spa.Transcode(xp.RETINA_input,output_vocab = vocab_memory)\n",
    "        G_input = spa.Transcode(xp.G_input, output_vocab = vocab_memory)\n",
    "        CORRECT_PRIM_input = spa.Transcode(xp.CORRECT_PRIM_input, output_vocab = prim_vocab)\n",
    "\n",
    "    nengo.Connection(RETINA_input.output, RETINA.input, synapse=None)\n",
    "    G_input >> G\n",
    "    CORRECT_PRIM_input >> CORRECT_PRIM\n",
    "    \n",
    "    \n",
    "    learning_inhibit = nengo.Node(output = xp.learning_inhibit_input)\n",
    "    PRIM_inhibit = spa.Scalar(label='PRIMSdone')\n",
    "    for ens in error.all_ensembles:\n",
    "        nengo.Connection(learning_inhibit, ens.neurons, transform=np.ones((ens.n_neurons, 1)) * 10, synapse=None)\n",
    "        nengo.Connection(PRIM_inhibit.output, ens.neurons, transform=np.ones((ens.n_neurons, 1)) , synapse=None)\n",
    "\n",
    "    # Definition of the actions\n",
    "    # There are rules that carry out the actions, and rules that check the\n",
    "    # conditions. If a condition is satisfied, check is set to YES which\n",
    "    # is a condition for the actions.\n",
    "    with spa.Network(label='BG-Thalamus') :\n",
    "        with spa.ActionSelection() as bg_thalamus:\n",
    "            # Action rules first\n",
    "            spa.ifmax( spa.dot(PRIM, s.V_COM),\n",
    "                        V >> COM_input,\n",
    "                        s.V_COM >> PREV\n",
    "                     )\n",
    "            spa.ifmax( spa.dot(PRIM, s.COM_PM),\n",
    "                        COM.output >> PM,\n",
    "                        s.COM_PM >> PREV\n",
    "                     )\n",
    "            spa.ifmax( spa.dot(PRIM, s.V_ADD),\n",
    "                        V >> ADD_input,\n",
    "                        s.V_ADD >> PREV\n",
    "                     )\n",
    "            spa.ifmax( spa.dot(PRIM, s.V_SUB),\n",
    "                        V >> SUB_input,\n",
    "                        s.V_SUB >> PREV\n",
    "                     )\n",
    "            spa.ifmax( spa.dot(PRIM, s.ADD_COM),\n",
    "                        ADD.output >> COM_input,\n",
    "                        s.ADD_COM >> PREV\n",
    "                     )\n",
    "            spa.ifmax( spa.dot(PRIM, s.SUB_COM),\n",
    "                        SUB.output >> COM_input,\n",
    "                        s.SUB_COM >> PREV\n",
    "                     )\n",
    "            spa.ifmax( spa.dot(PRIM, s.FOCUS),\n",
    "                        s.FOCUS >> PREV\n",
    "                     )\n",
    "            spa.ifmax(ROUTING_THR) # Threshold for action\n",
    "    \n",
    "    \n",
    "    with spa.Network(label='Inhibit- BG-Thalamus'):\n",
    "        with spa.ActionSelection():            \n",
    "            spa.ifmax( spa.dot(CORRECT_PRIM, s.V_COM) * spa.dot(V,COM),\n",
    "                        -1.0 >> PRIM_inhibit)\n",
    "            spa.ifmax( spa.dot(CORRECT_PRIM, s.COM_PM) * spa.dot(COM, PM),\n",
    "                        -1.0 >> PRIM_inhibit)\n",
    "            spa.ifmax( spa.dot(CORRECT_PRIM, s.V_ADD) * spa.dot(V,ADD),\n",
    "                        -1.0 >> PRIM_inhibit)\n",
    "            spa.ifmax( spa.dot(CORRECT_PRIM, s.V_SUB) * spa.dot(V,SUB),\n",
    "                        -1.0 >> PRIM_inhibit)\n",
    "            spa.ifmax( spa.dot(CORRECT_PRIM, s.ADD_COM) * spa.dot(ADD,COM),\n",
    "                        -1.0 >> PRIM_inhibit)\n",
    "            spa.ifmax( spa.dot(CORRECT_PRIM, s.SUB_COM) * spa.dot(SUB,COM),\n",
    "                        -1.0 >> PRIM_inhibit)\n",
    "            \n",
    "            spa.ifmax(ROUTING_THR, 0.0 >> PRIM_inhibit)\n",
    "         \n",
    "    with spa.Network(label='Action'):\n",
    "        with spa.ActionSelection():            \n",
    "                spa.ifmax( spa.dot(PM, s.MORE),\n",
    "                            s.MORE >> ACT)\n",
    "                spa.ifmax( spa.dot(PM, s.LESS),\n",
    "                            s.LESS >> ACT)\n",
    "\n",
    "                spa.ifmax( AM_THR)\n",
    "            \n",
    "    BTN = nengo.Node(Button([vocab_memory.parse('MORE').v, vocab_memory.parse('LESS').v], trial_length), size_in=D)\n",
    "    nengo.Connection(ACT.output, BTN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up some probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82640\n"
     ]
    }
   ],
   "source": [
    "with model:\n",
    "    \n",
    "    probe_dt = .05\n",
    "    #p_V = nengo.Probe(V.output, synapse = 0.01)#, sample_every = probe_dt)\n",
    "    #p_G = nengo.Probe(G.output, synapse = 0.01)#, sample_every = probe_dt)\n",
    "\n",
    "    p_PRIM = nengo.Probe(PRIM.output, synapse = 0.01)#, sample_every = probe_dt)\n",
    "    #p_PREV = nengo.Probe(PREV.output, synapse = 0.01)#, sample_every = probe_dt)\n",
    "\n",
    "    p_ADD = nengo.Probe(ADD.output, synapse = 0.01)#, sample_every = probe_dt)\n",
    "    p_SUB = nengo.Probe(SUB.output, synapse = 0.01)#, sample_every = probe_dt)\n",
    "    p_COM = nengo.Probe(COM.output, synapse = 0.01)#, sample_every = probe_dt)\n",
    "    \n",
    "    #p_PM = nengo.Probe(PM.output, synapse = 0.01)#, sample_every = probe_dt)\n",
    "    p_ACT = nengo.Probe(ACT.output, synapse = 0.01)#, sample_every = probe_dt)\n",
    "    p_BTN = nengo.Probe(BTN)#, sample_every = probe_dt)\n",
    "    \n",
    "    \n",
    "    print(model.n_neurons)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id=\"c566e591-752f-4d60-b503-beb5e2d9e666\" style=\"\n",
       "                    width: 100%;\n",
       "                    border: 1px solid #cfcfcf;\n",
       "                    border-radius: 4px;\n",
       "                    text-align: center;\n",
       "                    position: relative;\">\n",
       "                  <div class=\"pb-text\" style=\"\n",
       "                      position: absolute;\n",
       "                      width: 100%;\">\n",
       "                    0%\n",
       "                  </div>\n",
       "                  <div class=\"pb-fill\" style=\"\n",
       "                      background-color: #bdd2e6;\n",
       "                      width: 0%;\">\n",
       "                    <style type=\"text/css\" scoped=\"scoped\">\n",
       "                        @keyframes pb-fill-anim {\n",
       "                            0% { background-position: 0 0; }\n",
       "                            100% { background-position: 100px 0; }\n",
       "                        }\n",
       "                    </style>\n",
       "                    &nbsp;\n",
       "                  </div>\n",
       "                </div>"
      ],
      "text/plain": [
       "HtmlProgressBar cannot be displayed. Please use the TerminalProgressBar. It can be enabled with `nengo.rc.set('progress', 'progress_bar', 'nengo.utils.progress.TerminalProgressBar')`."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "              (function () {\n",
       "                  var root = document.getElementById('c566e591-752f-4d60-b503-beb5e2d9e666');\n",
       "                  var text = root.getElementsByClassName('pb-text')[0];\n",
       "                  var fill = root.getElementsByClassName('pb-fill')[0];\n",
       "\n",
       "                  text.innerHTML = 'Simulation finished in 0:01:00.';\n",
       "                  \n",
       "            if (100.0 > 0.) {\n",
       "                fill.style.transition = 'width 0.1s linear';\n",
       "            } else {\n",
       "                fill.style.transition = 'none';\n",
       "            }\n",
       "\n",
       "            fill.style.width = '100.0%';\n",
       "            fill.style.animation = 'none';\n",
       "            fill.style.backgroundImage = 'none'\n",
       "        \n",
       "                  \n",
       "                fill.style.animation = 'none';\n",
       "                fill.style.backgroundImage = 'none';\n",
       "            \n",
       "              })();\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dt = .001\n",
    "with simulator(model, dt = dt, seed=seed) as sim:\n",
    "    sim.run(T)\n",
    "    \n",
    "if save_weights:\n",
    "    for ws in weightSavers:\n",
    "        ws.save(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p_V' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-58503995f3e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0msubplot_i\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplot_similarities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp_V\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TWO'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'FOUR'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'SIX'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'EIGHT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'p_V'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplot_nrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplot_nrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplot_ncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplot_ncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0msubplot_i\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplot_similarities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp_G\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SIMPLE'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'CHAINED_SUB'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'CHAINED_ADD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'p_G'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplot_i\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplot_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplot_nrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplot_nrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplot_ncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplot_ncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0msubplot_i\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplot_similarities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp_PRIM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprim_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'p_PRIM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplot_i\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplot_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplot_nrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplot_nrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplot_ncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplot_ncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p_V' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x3240 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if True:\n",
    "    \n",
    "    def plot_similarities(t_range, data, vocab, keys=False, autoscale=True, title='Similarity', sort_legend=True, permutation=None, subplot_nrows=0, subplot_ncols=0, subplot_i = 1):\n",
    "\n",
    "        if not keys:\n",
    "            keys = list(vocab.keys())\n",
    "\n",
    "        if subplot_nrows * subplot_ncols > 0:\n",
    "            plt.subplot(subplot_nrows,subplot_ncols,subplot_i)\n",
    "\n",
    "        if permutation is None:\n",
    "            permutation = range(vocab.dimensions)\n",
    "        vectors = np.array([vocab.parse(p).v @ np.identity(vocab.dimensions)[permutation] for p in keys])\n",
    "        mean_activation = spa.similarity(data, vectors).mean(axis=0)\n",
    "        sort_idx = np.argsort(mean_activation)[::-1]    \n",
    "\n",
    "        ymin, ymax = -1.2, 1.2\n",
    "        plt.ylim(ymin, ymax)\n",
    "        plt.autoscale(autoscale, axis='y')\n",
    "        plt.grid(True)\n",
    "        plt.plot(t_range, spa.similarity(data, vectors[sort_idx]))\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Similarity\")\n",
    "        plt.xlim(left=t_range[0], right=t_range[-1])\n",
    "        plt.xticks(np.arange(t_range[0], t_range[-1], trial_length))\n",
    "        leg = plt.legend([str(round(mean_activation[sort_idx][i],2))+' '+k for i,k in enumerate(np.array(keys)[sort_idx])], loc='upper center',\n",
    "                   bbox_to_anchor=(0.5, -0.13), ncol=3)\n",
    "        \n",
    "        # set the linewidth of each legend object\n",
    "        for legobj in leg.legendHandles:\n",
    "            legobj.set_linewidth(4.0)\n",
    "            \n",
    "        if subplot_nrows * subplot_ncols == 0:\n",
    "            plt.show()\n",
    "\n",
    "        return subplot_i + 1\n",
    "\n",
    "\n",
    "\n",
    "    subplot_nrows=10\n",
    "    subplot_ncols=1\n",
    "    plt.figure(figsize=(6*subplot_ncols,4.5*subplot_nrows))\n",
    "    \n",
    "    start = 0#(number_of_total_trials-10)*trial_length\n",
    "    end = T\n",
    "    skip = 5\n",
    "    trange = sim.trange()\n",
    "    selected_idx = np.where(np.logical_and(trange > start, trange < end))\n",
    "    trange = trange[selected_idx][::skip]\n",
    "\n",
    "\n",
    "    subplot_i=plot_similarities(trange, sim.data[p_V][selected_idx][::skip], vocab_memory, keys=['TWO','FOUR','SIX','EIGHT'], title='p_V', subplot_nrows=subplot_nrows, subplot_ncols=subplot_ncols)\n",
    "    #subplot_i=plot_similarities(trange, sim.data[p_V][selected_idx][::skip], vocab_memory, keys=['TWO','FOUR','SIX','EIGHT'], title='p_V', subplot_nrows=subplot_nrows, subplot_ncols=subplot_ncols)\n",
    "    subplot_i=plot_similarities(trange, sim.data[p_G][selected_idx][::skip], vocab_memory, keys=['SIMPLE','CHAINED_SUB','CHAINED_ADD'], title='p_G', subplot_i=subplot_i, subplot_nrows=subplot_nrows, subplot_ncols=subplot_ncols)\n",
    "    subplot_i=plot_similarities(trange, sim.data[p_PRIM][selected_idx][::skip], prim_vocab, title='p_PRIM', subplot_i=subplot_i, subplot_nrows=subplot_nrows, subplot_ncols=subplot_ncols)\n",
    "    subplot_i=plot_similarities(trange, sim.data[p_PREV][selected_idx][::skip], prim_vocab, title='p_PREV', subplot_i=subplot_i, subplot_nrows=subplot_nrows, subplot_ncols=subplot_ncols)\n",
    "    subplot_i=plot_similarities(trange, sim.data[p_ADD][selected_idx][::skip], vocab_memory, keys=['TWO','FOUR','SIX','EIGHT'], title='p_ADD', subplot_i=subplot_i, subplot_nrows=subplot_nrows, subplot_ncols=subplot_ncols)\n",
    "    subplot_i=plot_similarities(trange, sim.data[p_SUB][selected_idx][::skip], vocab_memory, keys=['TWO','FOUR','SIX','EIGHT'], title='p_SUB', subplot_i=subplot_i, subplot_nrows=subplot_nrows, subplot_ncols=subplot_ncols)\n",
    "    subplot_i=plot_similarities(trange, sim.data[p_COM][selected_idx][::skip], vocab_memory, keys=['MORE','LESS'], title='p_COM', subplot_i=subplot_i, subplot_nrows=subplot_nrows, subplot_ncols=subplot_ncols)\n",
    "    subplot_i=plot_similarities(trange, sim.data[p_PM][selected_idx][::skip], vocab_memory, keys=['MORE','LESS'], title='p_PM', subplot_i=subplot_i, subplot_nrows=subplot_nrows, subplot_ncols=subplot_ncols)\n",
    "    subplot_i=plot_similarities(trange, sim.data[p_ACT][selected_idx][::skip], vocab_memory, keys=['MORE','LESS'], title='p_ACT', subplot_i=subplot_i, subplot_nrows=subplot_nrows, subplot_ncols=subplot_ncols)\n",
    "\n",
    "    trange = sim.trange()[selected_idx]\n",
    "    plt.subplot(subplot_nrows,subplot_ncols,subplot_i)\n",
    "    plt.plot(trange, sim.data[p_BTN][selected_idx])\n",
    "    plt.xlim(left=trange[0], right=trange[-1])\n",
    "    plt.xticks(np.arange(trange[0], trange[-1], trial_length))\n",
    "    plt.ylabel(\"Action\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score: 11/12\n",
      "Score after learning: 11/40\n"
     ]
    }
   ],
   "source": [
    "key_to_int = {'TWO':2, 'FOUR':4, 'SIX':6, 'EIGHT':8}\n",
    "COMresult_to_action = {True:'MORE', False:'LESS'}\n",
    "\n",
    "def get_expected_action(trial):\n",
    "    N = key_to_int[trial.stimulus]   \n",
    "    if trial.operation == 'CHAINED_ADD':\n",
    "        N += 2\n",
    "    elif trial.operation == 'CHAINED_SUB':\n",
    "        N -= 2\n",
    "    if N > 8:\n",
    "        N = 2\n",
    "    elif N < 2:\n",
    "        N = 8\n",
    "    expected_action = 1 + int(not N > 5)\n",
    "    return expected_action\n",
    "\n",
    "correct = []\n",
    "\n",
    "t = 0\n",
    "while t<T-.01:\n",
    "    \n",
    "    # Find the expected action\n",
    "    t += trial_length\n",
    "    expected_action = get_expected_action(xp(t)[0])\n",
    "    action_idx = (np.where(np.logical_and(sim.trange() < t, sim.trange() > t-trial_length))[0],)\n",
    "    model_action = sim.data[p_BTN][action_idx]\n",
    "    if np.count_nonzero(model_action) > 1:\n",
    "        print(\"ERROR: more than one action\")\n",
    "    model_action = model_action.sum()\n",
    "    correct += [model_action==expected_action]\n",
    "\n",
    "\n",
    "\n",
    "correct = np.array(correct, dtype=bool)\n",
    "test_correct = correct[-number_of_non_learning_trials:]\n",
    "\n",
    "plt.eventplot(np.where(np.logical_not(correct)))\n",
    "plt.xlim(-1,len(correct))\n",
    "plt.show()\n",
    "\n",
    "plt.eventplot(np.where(np.logical_not(test_correct)))\n",
    "plt.xlim(-1,len(test_correct))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('Total score: '+str(correct.sum())+'/'+str(number_of_total_trials))\n",
    "print('Score after learning: '+str(test_correct.sum())+'/'+str(number_of_non_learning_trials))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
