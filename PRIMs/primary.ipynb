{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nengo\n",
    "import nengo_spa as spa\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import random\n",
    "from weight_save import WeightSaver\n",
    "\n",
    "\n",
    "use_ocl = True\n",
    "if use_ocl:\n",
    "    import nengo_ocl\n",
    "    simulator = nengo_ocl.Simulator\n",
    "else:\n",
    "    simulator = nengo.Simulator\n",
    "    \n",
    "import sys, os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trial():\n",
    "    def __init__(self, operation, stimulus):\n",
    "        self.operation = operation\n",
    "        self.stimulus = stimulus\n",
    "\n",
    "class Experiment():\n",
    "    def __init__(self, trial_length, number_of_learning_trials, trials):\n",
    "        self.trial_length = trial_length\n",
    "        self.number_of_learning_trials = number_of_learning_trials\n",
    "        self.trials = trials\n",
    "\n",
    "    def __call__(self, t):\n",
    "        t = round(t,4) - .001 # Avoid float problems\n",
    "        trial_number = math.floor(t / self.trial_length)\n",
    "        t_in_trial = t - trial_number * self.trial_length\n",
    "        trial = self.trials[trial_number]\n",
    "        return trial, t_in_trial\n",
    "\n",
    "    \n",
    "    def RETINA_input(self, t):\n",
    "        trial, t_in_trial = self(t)\n",
    "        \n",
    "        if 1 < t_in_trial:# < 1.029:\n",
    "            return trial.stimulus\n",
    "        else:\n",
    "            return \"0\"\n",
    "\n",
    "    def G_input(self, t):\n",
    "        trial = self(t)[0]\n",
    "        return trial.operation\n",
    "\n",
    "    def CORRECT_PRIM_input(self, t):\n",
    "        trial, t_in_trial = self(t)\n",
    "\n",
    "        # before stimulus appears\n",
    "        if t_in_trial < 1:\n",
    "            return 'FOCUS'\n",
    "\n",
    "        else: # ['SIMPLE', 'CHAINED_ADD', 'CHAINED_SUB']\n",
    "            if trial.operation == 'SIMPLE':\n",
    "                if   1 < t_in_trial < 1.33:\n",
    "                    return 'V_COM'\n",
    "                elif 1.33 < t_in_trial < 1.66:\n",
    "                    return 'COM_PM'\n",
    "                else:\n",
    "                    return '0'\n",
    "\n",
    "            elif trial.operation == 'CHAINED_ADD':\n",
    "                if   1 < t_in_trial < 1.33:\n",
    "                    return 'V_ADD'\n",
    "                elif 1.33 < t_in_trial < 1.66:\n",
    "                    return 'ADD_COM'\n",
    "                elif 1.66 < t_in_trial < 1.99:\n",
    "                    return 'COM_PM'\n",
    "                else:\n",
    "                    return '0'\n",
    "\n",
    "            elif trial.operation == 'CHAINED_SUB':\n",
    "                if   1 < t_in_trial < 1.33:\n",
    "                    return 'V_SUB'\n",
    "                elif 1.33 < t_in_trial < 1.66:\n",
    "                    return 'SUB_COM'\n",
    "                elif 1.66 < t_in_trial < 1.99:\n",
    "                    return 'COM_PM'\n",
    "                return '0'\n",
    "\n",
    "            else:\n",
    "                print(\"unknown operation\")\n",
    "\n",
    "    def learning_inhibit_input(self, t):\n",
    "        trial_number = math.floor(t / self.trial_length)\n",
    "        if trial_number < self.number_of_learning_trials:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "class RandomExperiment(Experiment):\n",
    "    def __init__(self, trial_length, n_blocks_per_operation=10, n_trials_per_digit=5, n_different_digits=3, n_different_operations=4):\n",
    "        trials = []\n",
    "        for operation in ['SIMPLE', 'CHAINED_ADD', 'CHAINED_SUB'][:n_different_operations]:\n",
    "            for i in range(n_blocks_per_operation*n_trials_per_digit):\n",
    "                for stimulus in ['TWO', 'FOUR', 'SIX', 'EIGHT'][:n_different_digits]:\n",
    "                    trials.append(Trial(operation, stimulus))\n",
    "        shuffle(trials)\n",
    "\n",
    "        super().__init__(trial_length, number_of_learning_trials, trials)\n",
    "\n",
    "class Button():\n",
    "    def __init__(self, SP_vectors, trial_length, dt=None, thr=.5, focus_length=1):\n",
    "        self.t_last_evt = -100\n",
    "        self.SP_vectors = SP_vectors\n",
    "        self.t_last_step = 0\n",
    "        self.dt = dt\n",
    "        self.thr = thr\n",
    "        self.trial_length = trial_length\n",
    "        self.focus_length = focus_length\n",
    "    \n",
    "    def __call__(self,t,x):\n",
    "        if not self.dt or t-self.dt > self.t_last_step:\n",
    "            self.t_last_step = t\n",
    "            if t//self.trial_length > self.t_last_evt//self.trial_length and t > (t//self.trial_length)*self.trial_length + self.focus_length:\n",
    "                for i in range(len(self.SP_vectors)):\n",
    "                    similarities = np.dot(self.SP_vectors,x)\n",
    "                    if np.dot(x,self.SP_vectors[i]) > self.thr:\n",
    "                        self.t_last_evt = t\n",
    "                        return i+1\n",
    "                        \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_learning_trials 600\n",
      "number_of_non_learning_trials 60\n",
      "number_of_total_trials 660\n",
      "T 1339.13999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=12, similarity=0.12)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=13, similarity=0.14)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=14, similarity=0.13)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=15, similarity=0.15)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=16, similarity=0.14)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=17, similarity=0.12)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=18, similarity=0.17)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=19, similarity=0.19)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=20, similarity=0.16)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=21, similarity=0.18)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=22, similarity=0.20)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=23, similarity=0.20)\n",
      "  len(self._key2idx), best_sim))\n",
      "/home/ubuntu/anaconda3/envs/CTN/lib/python3.7/site-packages/nengo_spa/vocabulary.py:173: UserWarning: Could not create a semantic pointer with max_similarity=0.10 (D=32, M=24, similarity=0.18)\n",
      "  len(self._key2idx), best_sim))\n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "s = spa.sym\n",
    "D = 32  # the dimensionality of the vectors\n",
    "AM_THR = .15\n",
    "ROUTING_THR = .15\n",
    "GW_threshold = 0\n",
    "\n",
    "# Number of neurons (per dimension or ensemble)\n",
    "scale_npds = 1\n",
    "npd_AM = int(50*scale_npds) # Default: 50\n",
    "npd_state = int(50*scale_npds) # Default: 50\n",
    "npd_BG = int(100*scale_npds) # Default: 100\n",
    "npd_thal1 = int(50*scale_npds) # Default: 50\n",
    "npd_thal2 = int(40*scale_npds) # Default: 40\n",
    "n_scalar = int(50*scale_npds) # Default: 50\n",
    "\n",
    "save_weights = True\n",
    "load_weights = True\n",
    "\n",
    "n_blocks_per_operation = 11 # default: 10\n",
    "n_trials_per_digit = 5 # default: 5\n",
    "n_different_digits = 4 # default: 4\n",
    "n_different_operations = 3 # default: 3\n",
    "\n",
    "number_of_total_trials = n_blocks_per_operation * n_trials_per_digit * n_different_digits * n_different_operations\n",
    "number_of_non_learning_trials = number_of_total_trials-600\n",
    "number_of_learning_trials = max(0,number_of_total_trials - number_of_non_learning_trials)\n",
    "print(\"number_of_learning_trials\",number_of_learning_trials) \n",
    "print(\"number_of_non_learning_trials\",number_of_non_learning_trials) \n",
    "print(\"number_of_total_trials\",number_of_total_trials)\n",
    "\n",
    "\n",
    "trial_length = 2.029\n",
    "\n",
    "T = number_of_total_trials * trial_length - .00001# simulations run a bit too long\n",
    "print('T',T)\n",
    "\n",
    "symbol_keys = ['TWO','FOUR','SIX','EIGHT','X', \\\n",
    "               'MORE','LESS', \\\n",
    "    'G', 'V', 'COM', 'ADD', 'SUB', 'PREV', 'PM', \\\n",
    "    'SIMPLE', 'CHAINED_ADD', 'CHAINED_SUB'\n",
    "    ]\n",
    "prim_keys = ['V_COM', 'COM_PM', 'V_ADD', 'V_SUB', 'ADD_COM', 'SUB_COM', 'V_PM', 'FOCUS']\n",
    "all_keys = symbol_keys + prim_keys\n",
    "vocab_memory = spa.Vocabulary(dimensions=D, name='all', pointer_gen=np.random.RandomState(seed))\n",
    "vocab_memory.populate(\";\".join(all_keys))\n",
    "prim_vocab = vocab_memory.create_subset(prim_keys)\n",
    "\n",
    "\n",
    "xp = RandomExperiment(trial_length, n_blocks_per_operation, n_trials_per_digit, n_different_digits, n_different_operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from file\n",
      "Loading weights from file\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "model = spa.Network(seed=seed)\n",
    "with model:\n",
    "    \n",
    "    model.config[spa.State].neurons_per_dimension = npd_state\n",
    "    #model.config[spa.WTAAssocMem].n_neurons = npd_AM # Doesn't work -> set for individual AM\n",
    "    model.config[spa.Scalar].n_neurons = n_scalar\n",
    "    model.config[spa.BasalGanglia].n_neurons_per_ensemble = npd_BG\n",
    "    model.config[spa.Thalamus].neurons_action = npd_thal1\n",
    "    model.config[spa.Thalamus].neurons_channel_dim = npd_thal1\n",
    "    model.config[spa.Thalamus].neurons_gate = npd_thal2\n",
    "\n",
    "    # We start defining the buffer slots in which information can\n",
    "    # be placed:\n",
    "    \n",
    "    # A slot for the goal/task\n",
    "    G = spa.State(vocab_memory, label='G')\n",
    "    \n",
    "    # A slot for the visual input (the digit N). Feedback is used for iconic memory (100-300ms)\n",
    "    RETINA = spa.WTAAssocMem(\n",
    "        0.1,\n",
    "        vocab_memory,\n",
    "        mapping=['TWO','FOUR','SIX','EIGHT','X'],\n",
    "        function=lambda x: x>0,\n",
    "        n_neurons = npd_AM\n",
    "    )\n",
    "    nengo.Connection(RETINA.input, RETINA.input, transform=.85, synapse=.005)\n",
    "    V = spa.State(vocab_memory, label='V')\n",
    "    nengo.Connection(RETINA.output, V.input, synapse=.055)\n",
    "    \n",
    "    # The previously executed PRIM\n",
    "    PREV = spa.State(vocab_memory, feedback=.95, feedback_synapse=.05, label='PREV')\n",
    "    \n",
    "    # A slot for the action (MORE or LESS)\n",
    "    PM = spa.State(vocab_memory, feedback=.8, feedback_synapse=.05, label='PM')\n",
    "    with nengo.Network() as ACT_net:\n",
    "        ACT_net.config[nengo.Ensemble].neuron_type = nengo.Direct()\n",
    "        ACT = spa.State(vocab_memory, label='ACT direct')\n",
    "\n",
    "    # An associative memory for the + operation\n",
    "    ADD_input = spa.State(vocab_memory, feedback=.8, feedback_synapse=.05, label='ADD_input')\n",
    "    ADD = spa.WTAAssocMem(threshold=AM_THR, \n",
    "        input_vocab=vocab_memory, mapping=\n",
    "        {\n",
    "            'TWO':'FOUR',\n",
    "            'FOUR':'SIX',\n",
    "            'SIX':'EIGHT',\n",
    "            'EIGHT':'TWO',\n",
    "        },\n",
    "        function=lambda x: x>0,\n",
    "        label='ADD',\n",
    "        n_neurons = npd_AM\n",
    "    )\n",
    "    ADD_input >> ADD.input\n",
    "    \n",
    "    # An associative memory for the - operation\n",
    "    SUB_input = spa.State(vocab_memory, feedback=.8, feedback_synapse=.05, label='SUB_input')\n",
    "    SUB = spa.WTAAssocMem(threshold=AM_THR, \n",
    "        input_vocab=vocab_memory, mapping=\n",
    "        {\n",
    "            'TWO':'EIGHT',\n",
    "            'FOUR':'TWO',\n",
    "            'SIX':'FOUR',\n",
    "            'EIGHT':'SIX',\n",
    "        },\n",
    "        function=lambda x: x>0,\n",
    "        label='SUB',\n",
    "        n_neurons = npd_AM\n",
    "    )\n",
    "    SUB_input >> SUB.input\n",
    "    \n",
    "    # An associative memory for the \"compare to 5\" operation\n",
    "    COM_input = spa.State(vocab_memory, feedback=.8, feedback_synapse=.05, label='COM_input')\n",
    "    COM = spa.WTAAssocMem(threshold=AM_THR, \n",
    "        input_vocab=vocab_memory, mapping=\n",
    "        {\n",
    "            'TWO':'LESS',\n",
    "            'FOUR':'LESS',\n",
    "            'SIX':'MORE',\n",
    "            'EIGHT':'MORE',\n",
    "        },\n",
    "        function=lambda x: x>0,\n",
    "        label='COM',\n",
    "        n_neurons = npd_AM\n",
    "    )\n",
    "    COM_input >> COM.input\n",
    "\n",
    "    # A slot that combines selected information from the processors\n",
    "    \"\"\"GW = spa.State(vocab_memory, neurons_per_dimension = 150, label='GW')\"\"\"\n",
    "    GW = spa.State(vocab_memory, neurons_per_dimension = 150, label='GW', represent_cc_identity=False)\n",
    "    processors = [G, V, PREV, PM, ADD, SUB, COM]\n",
    "    competition_keys = {\n",
    "        G: ['SIMPLE', 'CHAINED_ADD', 'CHAINED_SUB'],\n",
    "        V: ['TWO','FOUR','SIX','EIGHT','X'],\n",
    "        PREV: ['V_COM', 'COM_PM', 'V_ADD', 'V_SUB', 'ADD_COM', 'SUB_COM', 'V_PM', 'FOCUS'],\n",
    "        PM: ['MORE','LESS'],\n",
    "        ADD: ['TWO','FOUR','SIX','EIGHT'],\n",
    "        SUB: ['TWO','FOUR','SIX','EIGHT'],\n",
    "        COM: ['MORE','LESS'],\n",
    "    }\n",
    "    for processor in processors:\n",
    "        source = processor.output\n",
    "        if GW_threshold:\n",
    "            proc_threshold = spa.modules.WTAAssocMem(\n",
    "                GW_threshold,\n",
    "                vocab_memory,\n",
    "                mapping=competition_keys[processor],\n",
    "                function=lambda x: x>0,\n",
    "                n_neurons = npd_AM\n",
    "            )\n",
    "            processor >> proc_threshold.input\n",
    "            source = proc_threshold.output\n",
    "            \n",
    "        nengo.Connection(source, GW.input, \n",
    "            transform=vocab_memory.parse(processor.label).get_binding_matrix())\n",
    "\n",
    "    # The PRIM state will receive the PRIM to be executed based on GW\n",
    "    PRIM = spa.State(prim_vocab, label='PRIM')\n",
    "    \n",
    "    # We will do supervised learned, so this state will hold the correct PRIM to execute\n",
    "    CORRECT_PRIM = spa.State(prim_vocab, label='correct action')\n",
    "    \n",
    "    # This state will be used to calculate the error\n",
    "    error = spa.State(prim_vocab, label='error')\n",
    "    PRIM - CORRECT_PRIM >> error\n",
    "    \n",
    "    \n",
    "    # Create learning connection between GW and PRIM\n",
    "    weightSavers = []\n",
    "    folder_name = \"final_weights\"\n",
    "    for i,ens in enumerate(GW.all_ensembles):\n",
    "        \n",
    "        con = nengo.Connection(ens, PRIM.input, function=lambda t: [0]*D,\n",
    "                               learning_rule_type=nengo.PES(learning_rate=1e-4)) # was 1e-4 and changed to 1e-5\n",
    "        nengo.Connection(error.output, con.learning_rule)\n",
    "        ws = WeightSaver(con, folder_name+'/w'+str(i), load=load_weights, sample_every=T)\n",
    "        weightSavers.append(ws)\n",
    "    \n",
    "    # Create the inputs\n",
    "    with spa.Network(label='inputs'):\n",
    "        RETINA_input = spa.Transcode(xp.RETINA_input,output_vocab = vocab_memory)\n",
    "        G_input = spa.Transcode(xp.G_input, output_vocab = vocab_memory)\n",
    "        CORRECT_PRIM_input = spa.Transcode(xp.CORRECT_PRIM_input, output_vocab = prim_vocab)\n",
    "\n",
    "    nengo.Connection(RETINA_input.output, RETINA.input, synapse=None)\n",
    "    G_input >> G\n",
    "    CORRECT_PRIM_input >> CORRECT_PRIM\n",
    "    \n",
    "    \n",
    "    learning_inhibit = nengo.Node(output = xp.learning_inhibit_input)\n",
    "    PRIM_inhibit = spa.Scalar(label='PRIMSdone')\n",
    "    for ens in error.all_ensembles:\n",
    "        nengo.Connection(learning_inhibit, ens.neurons, transform=np.ones((ens.n_neurons, 1)) * 10, synapse=None)\n",
    "        nengo.Connection(PRIM_inhibit.output, ens.neurons, transform=np.ones((ens.n_neurons, 1)) , synapse=None)\n",
    "\n",
    "    # Definition of the actions\n",
    "    # There are rules that carry out the actions, and rules that check the\n",
    "    # conditions. If a condition is satisfied, check is set to YES which\n",
    "    # is a condition for the actions.\n",
    "    with spa.Network(label='BG-Thalamus') :\n",
    "        with spa.ActionSelection() as bg_thalamus:\n",
    "            # Action rules first\n",
    "            spa.ifmax( spa.dot(PRIM, s.V_COM),\n",
    "                        V >> COM_input,\n",
    "                        s.V_COM >> PREV\n",
    "                     )\n",
    "            spa.ifmax( spa.dot(PRIM, s.COM_PM),\n",
    "                        COM.output >> PM,\n",
    "                        s.COM_PM >> PREV\n",
    "                     )\n",
    "            spa.ifmax( spa.dot(PRIM, s.V_ADD),\n",
    "                        V >> ADD_input,\n",
    "                        s.V_ADD >> PREV\n",
    "                     )\n",
    "            spa.ifmax( spa.dot(PRIM, s.V_SUB),\n",
    "                        V >> SUB_input,\n",
    "                        s.V_SUB >> PREV\n",
    "                     )\n",
    "            spa.ifmax( spa.dot(PRIM, s.ADD_COM),\n",
    "                        ADD.output >> COM_input,\n",
    "                        s.ADD_COM >> PREV\n",
    "                     )\n",
    "            spa.ifmax( spa.dot(PRIM, s.SUB_COM),\n",
    "                        SUB.output >> COM_input,\n",
    "                        s.SUB_COM >> PREV\n",
    "                     )\n",
    "            spa.ifmax( spa.dot(PRIM, s.FOCUS),\n",
    "                        s.FOCUS >> PREV\n",
    "                     )\n",
    "            spa.ifmax(ROUTING_THR) # Threshold for action\n",
    "    \n",
    "    \n",
    "    with spa.Network(label='Inhibit- BG-Thalamus'):\n",
    "        with spa.ActionSelection():            \n",
    "            spa.ifmax( spa.dot(CORRECT_PRIM, s.V_COM) * spa.dot(V,COM),\n",
    "                        -1.0 >> PRIM_inhibit)\n",
    "            spa.ifmax( spa.dot(CORRECT_PRIM, s.COM_PM) * spa.dot(COM, PM),\n",
    "                        -1.0 >> PRIM_inhibit)\n",
    "            spa.ifmax( spa.dot(CORRECT_PRIM, s.V_ADD) * spa.dot(V,ADD),\n",
    "                        -1.0 >> PRIM_inhibit)\n",
    "            spa.ifmax( spa.dot(CORRECT_PRIM, s.V_SUB) * spa.dot(V,SUB),\n",
    "                        -1.0 >> PRIM_inhibit)\n",
    "            spa.ifmax( spa.dot(CORRECT_PRIM, s.ADD_COM) * spa.dot(ADD,COM),\n",
    "                        -1.0 >> PRIM_inhibit)\n",
    "            spa.ifmax( spa.dot(CORRECT_PRIM, s.SUB_COM) * spa.dot(SUB,COM),\n",
    "                        -1.0 >> PRIM_inhibit)\n",
    "            \n",
    "            spa.ifmax(ROUTING_THR, 0.0 >> PRIM_inhibit)\n",
    "         \n",
    "    with spa.Network(label='Action'):\n",
    "        with spa.ActionSelection():            \n",
    "                spa.ifmax( spa.dot(PM, s.MORE),\n",
    "                            s.MORE >> ACT)\n",
    "                spa.ifmax( spa.dot(PM, s.LESS),\n",
    "                            s.LESS >> ACT)\n",
    "\n",
    "                spa.ifmax( AM_THR)\n",
    "            \n",
    "    BTN = nengo.Node(Button([vocab_memory.parse('MORE').v, vocab_memory.parse('LESS').v], trial_length), size_in=D)\n",
    "    nengo.Connection(ACT.output, BTN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up some probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82640\n"
     ]
    }
   ],
   "source": [
    "with model:\n",
    "    \n",
    "    probe_dt = .05\n",
    "    #p_V = nengo.Probe(V.output, synapse = 0.01)#, sample_every = probe_dt)\n",
    "    #p_G = nengo.Probe(G.output, synapse = 0.01)#, sample_every = probe_dt)\n",
    "\n",
    "    p_PRIM = nengo.Probe(PRIM.output, synapse = 0.01)#, sample_every = probe_dt)\n",
    "    #p_PREV = nengo.Probe(PREV.output, synapse = 0.01)#, sample_every = probe_dt)\n",
    "\n",
    "    p_ADD = nengo.Probe(ADD.output, synapse = 0.01)#, sample_every = probe_dt)\n",
    "    p_SUB = nengo.Probe(SUB.output, synapse = 0.01)#, sample_every = probe_dt)\n",
    "    p_COM = nengo.Probe(COM.output, synapse = 0.01)#, sample_every = probe_dt)\n",
    "    \n",
    "    #p_PM = nengo.Probe(PM.output, synapse = 0.01)#, sample_every = probe_dt)\n",
    "    p_ACT = nengo.Probe(ACT.output, synapse = 0.01)#, sample_every = probe_dt)\n",
    "    p_BTN = nengo.Probe(BTN)#, sample_every = probe_dt)\n",
    "    \n",
    "    \n",
    "    print(model.n_neurons)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No context argument was provided to nengo_ocl.Simulator\n",
      "Calling pyopencl.create_some_context() for you now:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Documents/Radboud/ERP/code/operations/weight_save.py:32: UserWarning: Weights file does not exist, initializing connection.\n",
      "  \"Weights file does not exist, initializing connection.\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id=\"6ab945bb-353b-4986-a871-fca99a7e6376\" style=\"\n",
       "                    width: 100%;\n",
       "                    border: 1px solid #cfcfcf;\n",
       "                    border-radius: 4px;\n",
       "                    text-align: center;\n",
       "                    position: relative;\">\n",
       "                  <div class=\"pb-text\" style=\"\n",
       "                      position: absolute;\n",
       "                      width: 100%;\">\n",
       "                    0%\n",
       "                  </div>\n",
       "                  <div class=\"pb-fill\" style=\"\n",
       "                      background-color: #bdd2e6;\n",
       "                      width: 0%;\">\n",
       "                    <style type=\"text/css\" scoped=\"scoped\">\n",
       "                        @keyframes pb-fill-anim {\n",
       "                            0% { background-position: 0 0; }\n",
       "                            100% { background-position: 100px 0; }\n",
       "                        }\n",
       "                    </style>\n",
       "                    &nbsp;\n",
       "                  </div>\n",
       "                </div>"
      ],
      "text/plain": [
       "HtmlProgressBar cannot be displayed. Please use the TerminalProgressBar. It can be enabled with `nengo.rc.set('progress', 'progress_bar', 'nengo.utils.progress.TerminalProgressBar')`."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "              (function () {\n",
       "                  var root = document.getElementById('6ab945bb-353b-4986-a871-fca99a7e6376');\n",
       "                  var text = root.getElementsByClassName('pb-text')[0];\n",
       "                  var fill = root.getElementsByClassName('pb-fill')[0];\n",
       "\n",
       "                  text.innerHTML = 'Simulation finished in 0:55:29.';\n",
       "                  \n",
       "            if (100.0 > 0.) {\n",
       "                fill.style.transition = 'width 0.1s linear';\n",
       "            } else {\n",
       "                fill.style.transition = 'none';\n",
       "            }\n",
       "\n",
       "            fill.style.width = '100.0%';\n",
       "            fill.style.animation = 'none';\n",
       "            fill.style.backgroundImage = 'none'\n",
       "        \n",
       "                  \n",
       "                fill.style.animation = 'none';\n",
       "                fill.style.backgroundImage = 'none';\n",
       "            \n",
       "              })();\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving weights to file\n",
      "(1, 32, 2400)\n",
      "Saving weights to file\n",
      "(1, 32, 2400)\n"
     ]
    }
   ],
   "source": [
    "dt = .001\n",
    "with simulator(model, dt = dt, seed=seed) as sim:\n",
    "    sim.run(T)\n",
    "    \n",
    "if save_weights:\n",
    "    for ws in weightSavers:\n",
    "        ws.save(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p_V' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-703061da0976>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0msubplot_i\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplot_similarities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp_V\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TWO'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'FOUR'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'SIX'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'EIGHT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'p_V'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplot_nrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplot_nrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplot_ncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplot_ncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;31m#subplot_i=plot_similarities(trange, sim.data[p_V][selected_idx][::skip], vocab_memory, keys=['TWO','FOUR','SIX','EIGHT'], title='p_V', subplot_nrows=subplot_nrows, subplot_ncols=subplot_ncols)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0msubplot_i\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplot_similarities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp_G\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SIMPLE'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'CHAINED_SUB'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'CHAINED_ADD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'p_G'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplot_i\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplot_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplot_nrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplot_nrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplot_ncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplot_ncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p_V' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x3240 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if False:\n",
    "    \n",
    "    def plot_similarities(t_range, data, vocab, keys=False, autoscale=True, title='Similarity', sort_legend=True, permutation=None, subplot_nrows=0, subplot_ncols=0, subplot_i = 1):\n",
    "\n",
    "        if not keys:\n",
    "            keys = list(vocab.keys())\n",
    "\n",
    "        if subplot_nrows * subplot_ncols > 0:\n",
    "            plt.subplot(subplot_nrows,subplot_ncols,subplot_i)\n",
    "\n",
    "        if permutation is None:\n",
    "            permutation = range(vocab.dimensions)\n",
    "        vectors = np.array([vocab.parse(p).v @ np.identity(vocab.dimensions)[permutation] for p in keys])\n",
    "        mean_activation = spa.similarity(data, vectors).mean(axis=0)\n",
    "        sort_idx = np.argsort(mean_activation)[::-1]    \n",
    "\n",
    "        ymin, ymax = -1.2, 1.2\n",
    "        plt.ylim(ymin, ymax)\n",
    "        plt.autoscale(autoscale, axis='y')\n",
    "        plt.grid(True)\n",
    "        plt.plot(t_range, spa.similarity(data, vectors[sort_idx]))\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Similarity\")\n",
    "        plt.xlim(left=t_range[0], right=t_range[-1])\n",
    "        plt.xticks(np.arange(t_range[0], t_range[-1], trial_length))\n",
    "        leg = plt.legend([str(round(mean_activation[sort_idx][i],2))+' '+k for i,k in enumerate(np.array(keys)[sort_idx])], loc='upper center',\n",
    "                   bbox_to_anchor=(0.5, -0.13), ncol=3)\n",
    "        \n",
    "        # set the linewidth of each legend object\n",
    "        for legobj in leg.legendHandles:\n",
    "            legobj.set_linewidth(4.0)\n",
    "            \n",
    "        if subplot_nrows * subplot_ncols == 0:\n",
    "            plt.show()\n",
    "\n",
    "        return subplot_i + 1\n",
    "\n",
    "\n",
    "\n",
    "    subplot_nrows=10\n",
    "    subplot_ncols=1\n",
    "    plt.figure(figsize=(6*subplot_ncols,4.5*subplot_nrows))\n",
    "    \n",
    "    start = 0#(number_of_total_trials-10)*trial_length\n",
    "    end = T\n",
    "    skip = 5\n",
    "    trange = sim.trange()\n",
    "    selected_idx = np.where(np.logical_and(trange > start, trange < end))\n",
    "    trange = trange[selected_idx][::skip]\n",
    "\n",
    "\n",
    "    subplot_i=plot_similarities(trange, sim.data[p_V][selected_idx][::skip], vocab_memory, keys=['TWO','FOUR','SIX','EIGHT'], title='p_V', subplot_nrows=subplot_nrows, subplot_ncols=subplot_ncols)\n",
    "    #subplot_i=plot_similarities(trange, sim.data[p_V][selected_idx][::skip], vocab_memory, keys=['TWO','FOUR','SIX','EIGHT'], title='p_V', subplot_nrows=subplot_nrows, subplot_ncols=subplot_ncols)\n",
    "    subplot_i=plot_similarities(trange, sim.data[p_G][selected_idx][::skip], vocab_memory, keys=['SIMPLE','CHAINED_SUB','CHAINED_ADD'], title='p_G', subplot_i=subplot_i, subplot_nrows=subplot_nrows, subplot_ncols=subplot_ncols)\n",
    "    subplot_i=plot_similarities(trange, sim.data[p_PRIM][selected_idx][::skip], prim_vocab, title='p_PRIM', subplot_i=subplot_i, subplot_nrows=subplot_nrows, subplot_ncols=subplot_ncols)\n",
    "    subplot_i=plot_similarities(trange, sim.data[p_PREV][selected_idx][::skip], prim_vocab, title='p_PREV', subplot_i=subplot_i, subplot_nrows=subplot_nrows, subplot_ncols=subplot_ncols)\n",
    "    subplot_i=plot_similarities(trange, sim.data[p_ADD][selected_idx][::skip], vocab_memory, keys=['TWO','FOUR','SIX','EIGHT'], title='p_ADD', subplot_i=subplot_i, subplot_nrows=subplot_nrows, subplot_ncols=subplot_ncols)\n",
    "    subplot_i=plot_similarities(trange, sim.data[p_SUB][selected_idx][::skip], vocab_memory, keys=['TWO','FOUR','SIX','EIGHT'], title='p_SUB', subplot_i=subplot_i, subplot_nrows=subplot_nrows, subplot_ncols=subplot_ncols)\n",
    "    subplot_i=plot_similarities(trange, sim.data[p_COM][selected_idx][::skip], vocab_memory, keys=['MORE','LESS'], title='p_COM', subplot_i=subplot_i, subplot_nrows=subplot_nrows, subplot_ncols=subplot_ncols)\n",
    "    subplot_i=plot_similarities(trange, sim.data[p_PM][selected_idx][::skip], vocab_memory, keys=['MORE','LESS'], title='p_PM', subplot_i=subplot_i, subplot_nrows=subplot_nrows, subplot_ncols=subplot_ncols)\n",
    "    subplot_i=plot_similarities(trange, sim.data[p_ACT][selected_idx][::skip], vocab_memory, keys=['MORE','LESS'], title='p_ACT', subplot_i=subplot_i, subplot_nrows=subplot_nrows, subplot_ncols=subplot_ncols)\n",
    "\n",
    "    trange = sim.trange()[selected_idx]\n",
    "    plt.subplot(subplot_nrows,subplot_ncols,subplot_i)\n",
    "    plt.plot(trange, sim.data[p_BTN][selected_idx])\n",
    "    plt.xlim(left=trange[0], right=trange[-1])\n",
    "    plt.xticks(np.arange(trange[0], trange[-1], trial_length))\n",
    "    plt.ylabel(\"Action\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score: 568/660\n",
      "Score after learning: 60/60\n"
     ]
    }
   ],
   "source": [
    "key_to_int = {'TWO':2, 'FOUR':4, 'SIX':6, 'EIGHT':8}\n",
    "COMresult_to_action = {True:'MORE', False:'LESS'}\n",
    "\n",
    "def get_expected_action(trial):\n",
    "    N = key_to_int[trial.stimulus] \n",
    "    #print(trial.operation, trial.stimulus)\n",
    "    if trial.operation == 'CHAINED_ADD':\n",
    "        N += 2\n",
    "    elif trial.operation == 'CHAINED_SUB':\n",
    "        N -= 2\n",
    "    if N > 8:\n",
    "        N = 2\n",
    "    elif N < 2:\n",
    "        N = 8\n",
    "    expected_action = 1 + int(not N > 5)\n",
    "    return expected_action\n",
    "\n",
    "correct = []\n",
    "\n",
    "t = 0\n",
    "while t<T-.01:\n",
    "    \n",
    "    # Find the expected action\n",
    "    t += trial_length\n",
    "    expected_action = get_expected_action(xp(t)[0])\n",
    "    action_idx = (np.where(np.logical_and(sim.trange() < t, sim.trange() > t-trial_length))[0],)\n",
    "    model_action = sim.data[p_BTN][action_idx]\n",
    "    if np.count_nonzero(model_action) > 1:\n",
    "        print(\"ERROR: more than one action\")\n",
    "    model_action = model_action.sum()\n",
    "    correct += [model_action==expected_action]\n",
    "\n",
    "\n",
    "\n",
    "correct = np.array(correct, dtype=bool)\n",
    "test_correct = correct[-number_of_non_learning_trials:]\n",
    "\n",
    "plt.eventplot(np.where(np.logical_not(correct)))\n",
    "plt.ylim(0.5,1.5)\n",
    "plt.xlim(-1,len(correct))\n",
    "plt.show()\n",
    "\n",
    "plt.eventplot(np.where(np.logical_not(test_correct)))\n",
    "plt.ylim(0.5,1.5)\n",
    "plt.xlim(-1,len(test_correct))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('Total score: '+str(correct.sum())+'/'+str(number_of_total_trials))\n",
    "print('Score after learning: '+str(test_correct.sum())+'/'+str(number_of_non_learning_trials))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
