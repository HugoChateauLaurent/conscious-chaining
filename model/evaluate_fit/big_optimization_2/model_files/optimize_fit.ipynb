{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nengo\n",
    "import nengo_spa as spa\n",
    "import pytry\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import random\n",
    "import sys, os\n",
    "import math\n",
    "from IPython import display\n",
    "from scipy.optimize import brute, minimize\n",
    "\n",
    "import skopt\n",
    "from skopt import gp_minimize\n",
    "from skopt.space.space import Integer, Real\n",
    "from skopt.plots import plot_evaluations, plot_objective\n",
    "\n",
    "from scipy.stats import sem\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.style as style\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "\n",
    "# Import our classes\n",
    "sys.path.append('..')\n",
    "import experiments as xps\n",
    "from experiments import create_xp\n",
    "from model import ExperimentRun, create_vocab\n",
    "from data import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization and model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = int(96)  # the dimensionality of the vectors\n",
    "PROC_FDBCK = .9\n",
    "PROC_FDBCK_SYN = .005\n",
    "GW_FDBCK = 1\n",
    "GW_SCALE = 20 # scale the input of GW for saturation and catching very short visual stimuli\n",
    "BG_THR = .1\n",
    "BG_BIAS = .5\n",
    "STIM_DURATION = .029\n",
    "FIX_DURATION = .5\n",
    "N_NEURONS_SCALE = 1\n",
    "N_NEURONS_SCALE_COMBINED = .25\n",
    "INTEGRATOR_RESET = False\n",
    "N_SAMPLES = 10000\n",
    "\n",
    "STARTING_SEED = 1\n",
    "N_SEEDS = 10\n",
    "\n",
    "N_BLOCKS_PER_OPERATION = 1 # default: 10\n",
    "N_TRIALS_PER_DIGIT = 2 # default: 5\n",
    "N_DIFFERENT_DIGITS = 4 # default: 4\n",
    "N_DIFFERENT_OPERATIONS = 3 # default: 3\n",
    "\n",
    "data_dir = \"evaluate_fit/big_optimization_2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: [0.15, 15, 0.1, 10000, 0.25, 0.9, 6, 20, 0.01]\n",
      "Simulating...\n",
      "\t seed 1/10\n",
      "running run_seed1_crosstalk0.15_sevid15_nsamples10000_combscale0.25_procfb0.9_D96_gwscale20_prevsyn0.01\n"
     ]
    }
   ],
   "source": [
    "def evaluation_function(params):\n",
    "    \n",
    "    print(\"params:\", params)\n",
    "    data = []\n",
    "    \n",
    "    D = params[6]*16\n",
    "    \n",
    "#     return 0\n",
    "        \n",
    "    print('Simulating...')\n",
    "    for i,seed in enumerate(range(STARTING_SEED, STARTING_SEED+N_SEEDS)):\n",
    "        print('\\t seed '+str(i+1)+'/'+str(N_SEEDS))\n",
    "        data_filename = 'run_seed'+str(seed)+'_crosstalk'+str(params[0])+'_sevid'+str(params[1])+'_nsamples'+str(params[3])+'_combscale'+str(params[4])+'_procfb'+str(params[5])+'_D'+str(D)+'_gwscale'+str(params[7])+'_prevsyn'+str(params[8])      \n",
    "        \n",
    "        try:\n",
    "            seed_data = pd.read_pickle(data_dir+'/'+data_filename+'.csv')\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            xp = create_xp(N_BLOCKS_PER_OPERATION, N_TRIALS_PER_DIGIT, N_DIFFERENT_DIGITS, N_DIFFERENT_OPERATIONS, STIM_DURATION, FIX_DURATION, seed)\n",
    "            results = ExperimentRun().run(\n",
    "                n_neurons_scale=N_NEURONS_SCALE,\n",
    "                n_neurons_scale_combined=params[4],\n",
    "                s_crosstalk=params[0],\n",
    "                s_evidence=params[1],\n",
    "                n_samples=int(params[3]),\n",
    "                t_senso=0,\n",
    "                vocab=create_vocab(D, seed),\n",
    "                xp=xp,\n",
    "                integrator_reset=INTEGRATOR_RESET,\n",
    "                proc_feedback=params[5],\n",
    "                proc_feedback_synapse=PROC_FDBCK_SYN,\n",
    "                prev_feedback_synapse=params[8],\n",
    "                GW_feedback=GW_FDBCK,\n",
    "                GW_scale=params[7],\n",
    "                BG_thr=BG_THR,\n",
    "                BG_bias=BG_BIAS,\n",
    "                seed=seed,\n",
    "                backend=\"nengo_ocl\",\n",
    "                data_dir=data_dir,\n",
    "                data_filename=data_filename,\n",
    "                plt=False\n",
    "            )\n",
    "\n",
    "            seed_data = results['data'].df\n",
    "\n",
    "            seed_data.to_pickle(data_dir+'/'+data_filename+'.csv')\n",
    "        data.append(seed_data)\n",
    "        \n",
    "        \n",
    "    data = Data(pd.concat(data))\n",
    "    data.df['rt'] += params[2]*1000\n",
    "    error = data.mean_differences_error(compare_errorrates=True, compare_RTs=True, tasks=range(N_DIFFERENT_OPERATIONS))\n",
    "    print('rmse:', error)\n",
    "    print('error rate:', data.error_rate)\n",
    "#     data.plot_fig2_simple(plot_humans=True)\n",
    "#     data.plot_fig2_chained(plot_humans=True)\n",
    "    return error\n",
    "\n",
    "res = gp_minimize(evaluation_function, \n",
    "    dimensions=[\n",
    "        Real(0, 1, name=\"crosstalk\"), # crosstalk\n",
    "        Real(5, 20, name=\"evidence strength\"), # evidence strength\n",
    "        Real(0, .2, name=\"senso\"), # sensory delay\n",
    "        Integer(100, 10000, name=\"number of samples\"), # number of samples\n",
    "        Real(.1, 1, name=\"scale combined\"), # n_neurons_scale_combined\n",
    "        Real(.5, 1, name=\"processor feedback\"), # processor feedback\n",
    "        Integer(2, 10, name=\"dimensions\"), # dimensions (*16)\n",
    "        Real(1, 30, name=\"GW scale\"), # GW scale\n",
    "        Real(.002, .02, name=\"PREV feedback synapse\"), # PREV feedback synapse\n",
    "    ],\n",
    "    x0=[\n",
    "        .15, # crosstalk\n",
    "        15, # evidence strength\n",
    "        .1, # sensory delay\n",
    "        N_SAMPLES,\n",
    "        N_NEURONS_SCALE_COMBINED,\n",
    "        PROC_FDBCK,\n",
    "        int(D/16),\n",
    "        GW_SCALE,\n",
    "        .01 # prev fb syn\n",
    "    ],\n",
    "    n_calls=100,\n",
    "    random_state=STARTING_SEED\n",
    "               \n",
    ")\n",
    "\n",
    "print(res)\n",
    "\n",
    "with open(data_dir+\"/res.pickle\", 'wb') as handle:\n",
    "    pickle.dump(res, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
